{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_boost import SketchBoost\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'helper' from '/home/davinci/term3/ml_proj/helper.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import helper\n",
    "reload(helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = './data/validation'\n",
    "label_dir = './data/validation_labels'\n",
    "\n",
    "y_MF, y_BP, y_CC = helper.load_dataset(image_dir, label_dir, cut_per_set=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2904, 489), (2904, 1943), (2904, 320))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_MF.shape, y_BP.shape, y_CC.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('./data/vectorization/validation.npz', allow_pickle=True)\n",
    "X = X['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = helper.split_dataset(X, y_BP, test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2032, 1720), (872, 1720))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play around with different `lr` and `gd_steps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:19:36] Stdout logging level is INFO.\n",
      "[21:19:36] GDBT train starts. Max iter 20000, early stopping rounds 1000\n",
      "[21:19:36] Iter 0; Sample 0, F1_score = 0.0; \n",
      "[21:20:02] Iter 1000; Sample 0, F1_score = 0.02305357720150229; \n",
      "[21:20:28] Iter 2000; Sample 0, F1_score = 0.02360407592352108; \n",
      "[21:20:53] Iter 3000; Sample 0, F1_score = 0.023907885599021537; \n",
      "[21:21:19] Iter 4000; Sample 0, F1_score = 0.025682312305476004; \n",
      "[21:21:44] Iter 5000; Sample 0, F1_score = 0.026198808118212923; \n",
      "[21:22:08] Iter 6000; Sample 0, F1_score = 0.02620830849288307; \n",
      "[21:22:33] Iter 7000; Sample 0, F1_score = 0.026311099548206776; \n",
      "[21:22:57] Iter 8000; Sample 0, F1_score = 0.0265534027349849; \n",
      "[21:23:23] Iter 9000; Sample 0, F1_score = 0.026601241274630302; \n",
      "[21:23:48] Iter 10000; Sample 0, F1_score = 0.026678989332000903; \n",
      "[21:24:12] Iter 11000; Sample 0, F1_score = 0.026740382141597987; \n",
      "[21:24:36] Iter 12000; Sample 0, F1_score = 0.027238117120430224; \n",
      "[21:25:02] Iter 13000; Sample 0, F1_score = 0.027343774408505286; \n",
      "[21:25:28] Iter 14000; Sample 0, F1_score = 0.027345238441757715; \n",
      "[21:25:45] Early stopping at iter 14736, best iter 13736, best_score 0.027357452629758496\n",
      "lr: 0.01, gd_steps: 5, F1 Score: 0.21306\n",
      "[21:25:53] Stdout logging level is INFO.\n",
      "[21:25:53] GDBT train starts. Max iter 20000, early stopping rounds 1000\n",
      "[21:25:53] Iter 0; Sample 0, F1_score = 0.0; \n",
      "[21:26:25] Iter 1000; Sample 0, F1_score = 0.023962305890276165; \n",
      "[21:26:57] Iter 2000; Sample 0, F1_score = 0.026197505047367724; \n",
      "[21:27:28] Iter 3000; Sample 0, F1_score = 0.027156626297188634; \n",
      "[21:27:59] Iter 4000; Sample 0, F1_score = 0.02748177886698498; \n",
      "[21:28:31] Iter 5000; Sample 0, F1_score = 0.027657479226965546; \n",
      "[21:29:02] Iter 6000; Sample 0, F1_score = 0.02768169188832152; \n",
      "[21:29:12] Early stopping at iter 6359, best iter 5359, best_score 0.02800797794018428\n",
      "lr: 0.01, gd_steps: 10, F1 Score: 0.21372\n",
      "[21:29:15] Stdout logging level is INFO.\n",
      "[21:29:15] GDBT train starts. Max iter 20000, early stopping rounds 1000\n",
      "[21:29:15] Iter 0; Sample 0, F1_score = 0.0; \n",
      "[21:29:59] Iter 1000; Sample 0, F1_score = 0.025152973461884637; \n",
      "[21:30:40] Iter 2000; Sample 0, F1_score = 0.026355020033020905; \n",
      "[21:31:23] Iter 3000; Sample 0, F1_score = 0.027281651267325648; \n",
      "[21:32:06] Iter 4000; Sample 0, F1_score = 0.027367786774745714; \n",
      "[21:32:48] Iter 5000; Sample 0, F1_score = 0.02752716762647233; \n",
      "[21:33:31] Iter 6000; Sample 0, F1_score = 0.02754462349910777; \n",
      "[21:34:12] Iter 7000; Sample 0, F1_score = 0.027541107298909427; \n",
      "[21:34:43] Early stopping at iter 7730, best iter 6730, best_score 0.02757020081527662\n",
      "lr: 0.01, gd_steps: 20, F1 Score: 0.20871\n",
      "[21:34:46] Stdout logging level is INFO.\n",
      "[21:34:46] GDBT train starts. Max iter 20000, early stopping rounds 1000\n",
      "[21:34:46] Iter 0; Sample 0, F1_score = 0.0; \n",
      "[21:35:13] Iter 1000; Sample 0, F1_score = 0.016035068955735175; \n",
      "[21:35:39] Iter 2000; Sample 0, F1_score = 0.017263228807894035; \n",
      "[21:36:05] Iter 3000; Sample 0, F1_score = 0.019601148280364637; \n",
      "[21:36:31] Iter 4000; Sample 0, F1_score = 0.020766034899126623; \n",
      "[21:36:57] Iter 5000; Sample 0, F1_score = 0.021204381533323696; \n",
      "[21:37:23] Iter 6000; Sample 0, F1_score = 0.02183512120529176; \n",
      "[21:37:48] Iter 7000; Sample 0, F1_score = 0.02217735712514888; \n",
      "[21:38:16] Iter 8000; Sample 0, F1_score = 0.022725701123192114; \n",
      "[21:38:41] Iter 9000; Sample 0, F1_score = 0.022975923728278315; \n",
      "[21:39:07] Iter 10000; Sample 0, F1_score = 0.023110406121782088; \n",
      "[21:39:32] Iter 11000; Sample 0, F1_score = 0.023128556953991204; \n",
      "[21:39:57] Iter 12000; Sample 0, F1_score = 0.023158306481307435; \n",
      "[21:40:20] Early stopping at iter 12848, best iter 11848, best_score 0.0231743085011786\n",
      "lr: 0.001, gd_steps: 5, F1 Score: 0.20731\n",
      "[21:40:23] Stdout logging level is INFO.\n",
      "[21:40:23] GDBT train starts. Max iter 20000, early stopping rounds 1000\n",
      "[21:40:23] Iter 0; Sample 0, F1_score = 0.0; \n",
      "[21:40:55] Iter 1000; Sample 0, F1_score = 0.017910866264680327; \n",
      "[21:41:27] Iter 2000; Sample 0, F1_score = 0.020767616178472132; \n",
      "[21:41:58] Iter 3000; Sample 0, F1_score = 0.021873886677909963; \n",
      "[21:42:31] Iter 4000; Sample 0, F1_score = 0.022139531579936665; \n",
      "[21:43:02] Iter 5000; Sample 0, F1_score = 0.022808927885330225; \n",
      "[21:43:34] Iter 6000; Sample 0, F1_score = 0.022892148285687097; \n",
      "[21:44:06] Iter 7000; Sample 0, F1_score = 0.023027377973080364; \n",
      "[21:44:38] Iter 8000; Sample 0, F1_score = 0.023110708310732638; \n",
      "[21:45:10] Iter 9000; Sample 0, F1_score = 0.023248809891086502; \n",
      "[21:45:36] Early stopping at iter 9830, best iter 8830, best_score 0.02326200121412734\n",
      "lr: 0.001, gd_steps: 10, F1 Score: 0.20915\n",
      "[21:45:40] Stdout logging level is INFO.\n",
      "[21:45:40] GDBT train starts. Max iter 20000, early stopping rounds 1000\n",
      "[21:45:40] Iter 0; Sample 0, F1_score = 0.0; \n",
      "[21:46:25] Iter 1000; Sample 0, F1_score = 0.02064349670951944; \n",
      "[21:47:08] Iter 2000; Sample 0, F1_score = 0.022092578430827136; \n",
      "[21:47:53] Iter 3000; Sample 0, F1_score = 0.0227049030071951; \n",
      "[21:48:39] Iter 4000; Sample 0, F1_score = 0.023206877977291665; \n",
      "[21:49:23] Iter 5000; Sample 0, F1_score = 0.02329106400658507; \n",
      "[21:50:05] Iter 6000; Sample 0, F1_score = 0.023524734055394722; \n",
      "[21:50:49] Iter 7000; Sample 0, F1_score = 0.024932214001353304; \n",
      "[21:51:32] Iter 8000; Sample 0, F1_score = 0.02534018098240989; \n",
      "[21:52:14] Iter 9000; Sample 0, F1_score = 0.025614160468114208; \n",
      "[21:52:58] Iter 10000; Sample 0, F1_score = 0.0256630076012209; \n",
      "[21:53:39] Iter 11000; Sample 0, F1_score = 0.025708471280930737; \n",
      "[21:53:45] Early stopping at iter 11124, best iter 10124, best_score 0.025721968263757104\n",
      "lr: 0.001, gd_steps: 20, F1 Score: 0.21097\n",
      "[21:53:49] Stdout logging level is INFO.\n",
      "[21:53:49] GDBT train starts. Max iter 20000, early stopping rounds 1000\n",
      "[21:53:49] Iter 0; Sample 0, F1_score = 0.0; \n",
      "[21:54:16] Iter 1000; Sample 0, F1_score = 0.0; \n",
      "[21:54:16] Early stopping at iter 1001, best iter 1, best_score 0.0\n",
      "lr: 0.0001, gd_steps: 5, F1 Score: 0.12736\n",
      "[21:54:16] Stdout logging level is INFO.\n",
      "[21:54:16] GDBT train starts. Max iter 20000, early stopping rounds 1000\n",
      "[21:54:16] Iter 0; Sample 0, F1_score = 0.0; \n",
      "[21:54:49] Iter 1000; Sample 0, F1_score = 0.0; \n",
      "[21:54:49] Early stopping at iter 1001, best iter 1, best_score 0.0\n",
      "lr: 0.0001, gd_steps: 10, F1 Score: 0.12736\n",
      "[21:54:49] Stdout logging level is INFO.\n",
      "[21:54:49] GDBT train starts. Max iter 20000, early stopping rounds 1000\n",
      "[21:54:49] Iter 0; Sample 0, F1_score = 0.0; \n",
      "[21:55:34] Iter 1000; Sample 0, F1_score = 0.0; \n",
      "[21:55:34] Early stopping at iter 1001, best iter 1, best_score 0.0\n",
      "lr: 0.0001, gd_steps: 20, F1 Score: 0.12736\n",
      "Best Parameters: {'lr': 0.01, 'gd_steps': 10}\n",
      "Best F1 Score: 0.21372\n"
     ]
    }
   ],
   "source": [
    "lr_values = [1e-2, 1e-3, 1e-4]\n",
    "gd_steps_values = [5, 10, 20]\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params = {}\n",
    "\n",
    "for lr in lr_values:\n",
    "    for gd_steps in gd_steps_values:\n",
    "        model_mf = SketchBoost(\n",
    "            loss='multilabel', metric='f1', ntrees=20_000,\n",
    "            lr=lr, es=1_000, lambda_l2=1, gd_steps=gd_steps,\n",
    "            min_data_in_leaf=10, max_bin=256, max_depth=5,\n",
    "            verbose=1_000\n",
    "        )\n",
    "\n",
    "        model_mf.fit(X_train, y_train, eval_sets=[{'X': X_test, 'y': y_test}])\n",
    "\n",
    "        y_pred = model_mf.predict(np.array(X_test))\n",
    "        score = helper.count_f1_max(y_pred, y_test)\n",
    "        print(f\"lr: {lr}, gd_steps: {gd_steps}, F1 Score: {score:.5f}\")\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = {'lr': lr, 'gd_steps': gd_steps}\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best F1 Score: {best_score:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since best params are `lr = 0.01` and `gd_steps = 10` we will use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:04:46] Stdout logging level is INFO.\n",
      "[22:04:46] GDBT train starts. Max iter 20000, early stopping rounds 1000\n",
      "[22:04:46] Iter 0; Sample 0, F1_score = 0.0; \n",
      "[22:05:18] Iter 1000; Sample 0, F1_score = 0.023958054620118796; \n",
      "[22:05:50] Iter 2000; Sample 0, F1_score = 0.02624877893669352; \n",
      "[22:06:20] Iter 3000; Sample 0, F1_score = 0.02731273944178301; \n",
      "[22:06:52] Iter 4000; Sample 0, F1_score = 0.027519415673867877; \n",
      "[22:07:24] Iter 5000; Sample 0, F1_score = 0.02764096171108459; \n",
      "[22:07:55] Iter 6000; Sample 0, F1_score = 0.02782178168325158; \n",
      "[22:08:26] Iter 7000; Sample 0, F1_score = 0.0277995541155098; \n",
      "[22:08:42] Early stopping at iter 7482, best iter 6482, best_score 0.027832133049662353\n",
      "lr: 0.0001, gd_steps: 20, F1 Score: 0.21381\n"
     ]
    }
   ],
   "source": [
    "model_mf = SketchBoost(\n",
    "            loss='multilabel', metric='f1', ntrees=20_000,\n",
    "            lr=.01, es=1_000, lambda_l2=1, gd_steps=10,\n",
    "            min_data_in_leaf=10, max_bin=256, max_depth=5,\n",
    "            verbose=1_000\n",
    "        )\n",
    "\n",
    "model_mf.fit(X_train, y_train, eval_sets=[{'X': X_test, 'y': y_test}])\n",
    "\n",
    "y_pred = model_mf.predict(np.array(X_test))\n",
    "score = helper.count_f1_max(y_pred, y_test)\n",
    "print(f\"lr: {lr}, gd_steps: {gd_steps}, Score: {score:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyBoost with ICA dim red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if ICA increses score, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davinci/miniconda3/envs/py10/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n_components = 256\n",
    "pipeline_transform = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ica', FastICA(n_components=n_components, random_state=42, max_iter=2_000)),\n",
    "])\n",
    "\n",
    "pipeline_transform.fit(X_train)\n",
    "\n",
    "X_train_transformed = pipeline_transform.transform(X_train)\n",
    "X_test_transformed = pipeline_transform.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:13:37] Stdout logging level is INFO.\n",
      "[22:13:37] GDBT train starts. Max iter 20000, early stopping rounds 1000\n",
      "[22:13:38] Iter 0; Sample 0, F1_score = 0.0; \n",
      "[22:14:02] Iter 1000; Sample 0, F1_score = 0.02127316340148577; \n",
      "[22:14:29] Iter 2000; Sample 0, F1_score = 0.021717544033652277; \n",
      "[22:14:53] Iter 3000; Sample 0, F1_score = 0.021862141923949797; \n",
      "[22:15:18] Iter 4000; Sample 0, F1_score = 0.022013651024497177; \n",
      "[22:15:42] Iter 5000; Sample 0, F1_score = 0.022054798613758655; \n",
      "[22:15:53] Early stopping at iter 5390, best iter 4390, best_score 0.02207241589716595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<py_boost.gpu.sketch_boost.SketchBoost at 0x7f08b8b6eec0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mf = SketchBoost(\n",
    "            loss='multilabel', metric='f1', ntrees=20_000,\n",
    "            lr=.01, es=1_000, lambda_l2=1, gd_steps=10,\n",
    "            min_data_in_leaf=10, max_bin=256, max_depth=5,\n",
    "            verbose=1_000\n",
    "        )\n",
    "\n",
    "model_mf.fit(X_train_transformed, y_train, eval_sets = [{'X': X_test_transformed, 'y': y_test}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10608\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_mf.predict(np.array(X_test))\n",
    "print(f\"{helper.count_f1_max(y_pred, y_test):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, ICA works best with classic ML then."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
